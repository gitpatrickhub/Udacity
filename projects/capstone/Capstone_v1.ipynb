{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ec2-user/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ec2-user/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# referenced the below site to get started\n",
    "# https://kaggle2.blob.core.windows.net/forum-message-attachments/118880/4192/main.py \n",
    "\n",
    "# import packages\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import cv2\n",
    "import math\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json \n",
    "\n",
    "from numpy.random import permutation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed and color_type\n",
    "# color_type = 1 - grayscale\n",
    "# color_type = 3 - RGB\n",
    "\n",
    "np.random.seed(2016)\n",
    "use_cache = 1\n",
    "\n",
    "color_type_global = 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check each image color type; grayscale or rgb and resize\n",
    "\n",
    "def get_im(path, img_rows, img_cols, color_type=1):\n",
    "    # Load as grayscale\n",
    "    if color_type == 1:\n",
    "        img = cv2.imread(path, 0)\n",
    "    elif color_type == 3:\n",
    "        img = cv2.imread(path)\n",
    "    # Reduce size 224, 224\n",
    "    resized = cv2.resize(img, (img_cols, img_rows))\n",
    "\n",
    "    return resized\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting images in folder c0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting images in folder c1\n",
      "Augmenting images in folder c2\n",
      "Augmenting images in folder c3\n",
      "Augmenting images in folder c4\n",
      "Augmenting images in folder c5\n",
      "Augmenting images in folder c6\n",
      "Augmenting images in folder c7\n",
      "Augmenting images in folder c8\n",
      "Augmenting images in folder c9\n"
     ]
    }
   ],
   "source": [
    "# add data augmentation\n",
    "import random\n",
    "from scipy import ndarray\n",
    "import skimage as sk\n",
    "from skimage import transform\n",
    "from skimage import util\n",
    "from skimage import io\n",
    "\n",
    "def img_rotation(image_array: ndarray):\n",
    "    # pick a random degree of rotation between 25% on the left and 25% on the right\n",
    "    random_degree = random.uniform(-25, 25)\n",
    "    return sk.transform.rotate(image_array, random_degree)\n",
    "\n",
    "def img_noise(image_array: ndarray):\n",
    "    # add random noise to the image\n",
    "    return sk.util.random_noise(image_array)\n",
    "\n",
    "img_rows = 224\n",
    "img_cols = 224\n",
    "color_type = 3\n",
    "\n",
    "for j in range(10):\n",
    "        print('Augmenting images in folder c{}'.format(j))\n",
    "        num_files_created = 0\n",
    "        path = os.path.join('Data', 'imgs', 'train', 'c' + str(j), '*.jpg')\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            flbase = os.path.basename(fl)\n",
    "            img = get_im(fl, img_rows, img_cols, color_type)\n",
    "            rotated_image = img_rotation(img)\n",
    "            \n",
    "            save_path = os.path.join('Data', 'imgs', 'train', 'c' + str(j))\n",
    "            new_file_path = '%s/augmented1_%s' % (save_path, flbase)\n",
    "\n",
    "            # save rotated image\n",
    "            io.imsave(new_file_path, rotated_image)\n",
    "            \n",
    "            noise_image = img_noise(img)\n",
    "            \n",
    "            save_path = os.path.join('Data', 'imgs', 'train', 'c' + str(j))\n",
    "            new_file_path = '%s/augmented2_%s' % (save_path, flbase)\n",
    "\n",
    "            # save rotated image\n",
    "            io.imsave(new_file_path, noise_image)\n",
    "            \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New driver file saved for augmented images\n"
     ]
    }
   ],
   "source": [
    "# add augmented images to driver data csv\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Data/driver_imgs_list.csv', header=0)\n",
    "df1 = df.copy()\n",
    "df2 = df.copy()\n",
    "\n",
    "df1.iloc[:,2] = 'augmented1_'+df1.iloc[:,2]          \n",
    "df2.iloc[:,2] = 'augmented2_'+df2.iloc[:,2]\n",
    "\n",
    "merged = df.append([df1,df2])\n",
    "merged.to_csv('Data/driver_imgs_list_merged.csv', encoding='utf-8', index=False)\n",
    "\n",
    "print(\"New driver file saved for augmented images\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load driver data from local drive\n",
    "def get_driver_data():\n",
    "    dr = dict()\n",
    "    path = os.path.join('Data', 'driver_imgs_list_merged.csv')\n",
    "    print('Read drivers data')\n",
    "    f = open(path, 'r')\n",
    "    line = f.readline()\n",
    "    while (1):\n",
    "        line = f.readline()\n",
    "        if line == '':\n",
    "            break\n",
    "        arr = line.strip().split(',')\n",
    "        dr[arr[2]] = arr[0]\n",
    "    f.close()\n",
    "    return dr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data from local drive\n",
    "\n",
    "def load_train(img_rows, img_cols, color_type=1):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    driver_id = []\n",
    "    driver_data = get_driver_data()\n",
    "\n",
    "    print('Read train images')\n",
    "    \n",
    "    # loop through 'c' folders, images and target data \n",
    "    for j in range(10):\n",
    "        print('Load folder c{}'.format(j))\n",
    "        path = os.path.join('Data', 'imgs', 'train', 'c' + str(j), '*.jpg')\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            flbase = os.path.basename(fl)\n",
    "            img = get_im(fl, img_rows, img_cols, color_type)\n",
    "            X_train.append(img)\n",
    "            y_train.append(j)\n",
    "            driver_id.append(driver_data[flbase])\n",
    "\n",
    "    unique_drivers = sorted(list(set(driver_id)))\n",
    "    print('Unique drivers: {}'.format(len(unique_drivers)))\n",
    "    print(unique_drivers)\n",
    "    return X_train, y_train, driver_id, unique_drivers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data from local drive\n",
    "\n",
    "def load_test(img_rows, img_cols, color_type=1):\n",
    "    print('Read test images')\n",
    "    path = os.path.join('Data', 'imgs', 'test', '*.jpg')\n",
    "    files = glob.glob(path)\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    total = 0\n",
    "    thr = math.floor(len(files)/10)\n",
    "    \n",
    "    # loop through test files and print status\n",
    "    \n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im(fl, img_rows, img_cols, color_type)\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(flbase)\n",
    "        total += 1\n",
    "        if total % thr == 0:\n",
    "            print('Read {} images from {}'.format(total, len(files)))\n",
    "\n",
    "    return X_test, X_test_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pickle file to enable faster processing\n",
    "\n",
    "def cache_data(data, path):\n",
    "    print('Path = '+path)\n",
    "    \n",
    "    if not os.path.isdir('cache'):\n",
    "        os.mkdir('cache')\n",
    "    if os.path.isdir(os.path.dirname(path)):\n",
    "        \n",
    "        # modifications for running on Mac OSX\n",
    "        #file1 = open(path, 'wb')\n",
    "        #pickle.dump(data, file1)\n",
    "        #file1.close()\n",
    "        \n",
    "        \n",
    "        max_bytes = 2**31 - 1\n",
    "        bytes_out = pickle.dumps(data)\n",
    "        n_bytes = sys.getsizeof(bytes_out)\n",
    "        with open(path, 'wb') as f_out:\n",
    "            for idx in range(0, n_bytes, max_bytes):\n",
    "                f_out.write(bytes_out[idx:idx+max_bytes])\n",
    "        print('Pickle complete')\n",
    "    else:\n",
    "        print('Directory doesnt exists')\n",
    "\n",
    "# restore data from pickle\n",
    "        \n",
    "def restore_data(path):\n",
    "    data = dict()\n",
    "    if os.path.isfile(path):\n",
    "        print('Restore data from pickle........')\n",
    "        \n",
    "        # modifications for running on Mac OSX\n",
    "        #file1 = open(path, 'rb')\n",
    "        #data = pickle.load(file1) \n",
    "        max_bytes = 2**31 - 1\n",
    "        try:\n",
    "            input_size = os.path.getsize(path)\n",
    "            bytes_in = bytearray(0)\n",
    "            with open(path, 'rb') as f_in:\n",
    "                for _ in range(0, input_size, max_bytes):\n",
    "                    bytes_in += f_in.read(max_bytes)\n",
    "            data = pickle.loads(bytes_in)\n",
    "            print('Finished loading Pickle data')\n",
    "        except:\n",
    "            print('Error loading Pickle data')\n",
    "            return None\n",
    "\n",
    "    return data\n",
    "\n",
    "# save model weights in json\n",
    "\n",
    "def save_model(model, index, cross=''):\n",
    "    json_string = model.to_json()\n",
    "    if not os.path.isdir('cache'):\n",
    "        os.mkdir('cache')\n",
    "    json_name = 'architecture' + str(index) + cross + '.json'\n",
    "    weight_name = 'model_weights' + str(index) + cross + '.h5'\n",
    "    open(os.path.join('cache', json_name), 'w').write(json_string)\n",
    "    model.save_weights(os.path.join('cache', weight_name), overwrite=True)\n",
    "    print('Model weights saved')\n",
    "\n",
    "# added load_weights paramenter by_name=True referenced link below\n",
    "# https://github.com/keras-team/keras/pull/8999/commits/fbd106a2dc6cc5fa17c64220d07c7520f7c0b044\n",
    "\n",
    "def read_model(index, cross=''):\n",
    "    print('Reading model weights')\n",
    "    json_name = 'architecture' + str(index) + cross + '.json'\n",
    "    weight_name = 'model_weights' + str(index) + cross + '.h5'\n",
    "    model = model_from_json(open(os.path.join('cache', json_name)).read())\n",
    "    model.load_weights(os.path.join('cache', weight_name), by_name=True)\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submisssion file needed for kaggle competition\n",
    "\n",
    "def create_submission(predictions, test_id, info):\n",
    "    result1 = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3',\n",
    "                                                 'c4', 'c5', 'c6', 'c7',\n",
    "                                                 'c8', 'c9'])\n",
    "    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\n",
    "    now = datetime.datetime.now()\n",
    "    if not os.path.isdir('subm'):\n",
    "        os.mkdir('subm')\n",
    "    suffix = info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "    sub_file = os.path.join('subm', 'submission_' + suffix + '.csv')\n",
    "    result1.to_csv(sub_file, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train data from local or from pickle and perform transformations\n",
    "    \n",
    "def read_and_normalize_and_shuffle_train_data(img_rows, img_cols,\n",
    "                                              color_type=1):\n",
    "\n",
    "    cache_path = os.path.join('cache', 'train_r_' + str(img_rows) +\n",
    "                              '_c_' + str(img_cols) + '_t_' +\n",
    "                              str(color_type) + '.dat')\n",
    "\n",
    "    if not os.path.isfile(cache_path) or use_cache == 0:\n",
    "        train_data, train_target, driver_id, unique_drivers = \\\n",
    "            load_train(img_rows, img_cols, color_type)\n",
    "        cache_data((train_data, train_target, driver_id, unique_drivers),\n",
    "                   cache_path)\n",
    "    else:\n",
    "        print('Restore train from cache!')\n",
    "        (train_data, train_target, driver_id, unique_drivers) = \\\n",
    "            restore_data(cache_path)\n",
    "\n",
    "    train_data = np.array(train_data, dtype=np.uint8)\n",
    "    train_target = np.array(train_target, dtype=np.uint8)\n",
    "\n",
    "    #if color_type == 1:\n",
    "    #   train_data = train_data.reshape(train_data.shape[0], color_type,\n",
    "    #                                  img_rows, img_cols)\n",
    "    # modified to channels_last format\n",
    "    \n",
    "    if color_type == 1:\n",
    "        train_data = train_data.reshape(train_data.shape[0],\n",
    "                                      img_rows, img_cols, color_type)\n",
    "    #else:\n",
    "    #   train_data = train_data.transpose((0, 3, 1, 2))\n",
    "    # modified to channels_last format\n",
    "    \n",
    "    else:\n",
    "        train_data = train_data.transpose((0, 1, 2, 3))\n",
    "\n",
    "    train_target = np_utils.to_categorical(train_target, 10)\n",
    "    train_data = train_data.astype('float32')\n",
    "    mean_pixel = [103.939, 116.779, 123.68]\n",
    "    for c in range(3):\n",
    "        train_data[:, c, :, :] = train_data[:, c, :, :] - mean_pixel[c]\n",
    "    # train_data /= 255\n",
    "    perm = permutation(len(train_target))\n",
    "    train_data = train_data[perm]\n",
    "    train_target = train_target[perm]\n",
    "    print('Train shape:', train_data.shape)\n",
    "    print(train_data.shape[0], 'train samples')\n",
    "    return train_data, train_target, driver_id, unique_drivers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and transform test data\n",
    "\n",
    "def read_and_normalize_test_data(img_rows=224, img_cols=224, color_type=1):\n",
    "    cache_path = os.path.join('cache', 'test_r_' + str(img_rows) +\n",
    "                              '_c_' + str(img_cols) + '_t_' +\n",
    "                              str(color_type) + '.dat')\n",
    "    if not os.path.isfile(cache_path) or use_cache == 0:\n",
    "        test_data, test_id = load_test(img_rows, img_cols, color_type)\n",
    "        cache_data((test_data, test_id), cache_path)\n",
    "    else:\n",
    "        print('Restore test from cache!')\n",
    "        (test_data, test_id) = restore_data(cache_path)\n",
    "\n",
    "    test_data = np.array(test_data, dtype=np.uint8)\n",
    "    \n",
    "    #if color_type == 1:\n",
    "    #   train_data = train_data.reshape(train_data.shape[0], color_type,\n",
    "    #                                  img_rows, img_cols)\n",
    "    # modified to channels_last format\n",
    "     \n",
    "    if color_type == 1:\n",
    "        test_data = test_data.reshape(test_data.shape[0],\n",
    "                                      img_rows, img_cols, color_type)\n",
    "    #else:\n",
    "    #   test_data = test_data.transpose(0, 3, 1, 2))\n",
    "    # modified to channels_last format\n",
    "    \n",
    "    else:\n",
    "        test_data = test_data.transpose((0, 1, 2, 3))\n",
    "\n",
    "    test_data = test_data.astype('float32')\n",
    "    mean_pixel = [103.939, 116.779, 123.68]\n",
    "    for c in range(3):\n",
    "        test_data[:, c, :, :] = test_data[:, c, :, :] - mean_pixel[c]\n",
    "    # test_data /= 255\n",
    "    print('Test shape:', test_data.shape)\n",
    "    print(test_data.shape[0], 'test samples')\n",
    "    return test_data, test_id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfolds means config\n",
    "\n",
    "def merge_several_folds_mean(data, nfolds):\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, nfolds):\n",
    "        a += np.array(data[i])\n",
    "    a /= nfolds\n",
    "    return a.tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created simplified approach since defined vgg16 model would take 180 hours (3 epochs * 2 folds * 180 = 1080) \n",
    "# per epoch on macbook pro cpu. \n",
    "# attempted on aws gpu instance, but ran into OOM issues\n",
    "# referenced the following site for transfer learning techniques \n",
    "# https://keras.io/applications/#vgg16\n",
    "\n",
    "from keras.applications.vgg16 import VGG16    \n",
    "from keras.models import Model\n",
    "\n",
    "def vgg16_model(img_rows, img_cols, color_type=1, num_classes=None):\n",
    "    model = VGG16(weights='imagenet', include_top=True, input_shape=((224, 224,3)))\n",
    "    model.layers.pop()\n",
    "    model.outputs = [model.layers[-1].output]\n",
    "    model.layers[-1].outbound_nodes = []\n",
    "    x=Dense(num_classes, activation='softmax')(model.output)\n",
    "    model=Model(model.input,x)\n",
    "\n",
    "# set first 8 layers to non-trainable\n",
    "\n",
    "    for layer in model.layers[:8]:\n",
    "       layer.trainable = False\n",
    "\n",
    "\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation, fit model, save model outputs\n",
    "\n",
    "def run_cross_validation(nfolds=2, nb_epoch=3, split=0.2, modelStr=''):\n",
    "\n",
    "    # Now it loads color image\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 224, 224\n",
    "    # reduced batch size from 64\n",
    "    batch_size = 128\n",
    "    random_state = 20\n",
    "\n",
    "    train_data, train_target, driver_id, unique_drivers = \\\n",
    "        read_and_normalize_and_shuffle_train_data(img_rows, img_cols,\n",
    "                                                  color_type_global)\n",
    "\n",
    "    num_fold = 0\n",
    "    kf = KFold(len(unique_drivers), n_folds=nfolds,\n",
    "               shuffle=True, random_state=random_state)\n",
    "    for train_drivers, test_drivers in kf:\n",
    "        num_fold += 1\n",
    "        print('Start KFold number {} of {}'.format(num_fold, nfolds))\n",
    "\n",
    "        # model = vgg_std16_model(img_rows, img_cols, color_type_global)\n",
    "        model = vgg16_model(img_rows, img_cols, color_type_global, 10)\n",
    "        \n",
    "        model.fit(train_data, train_target, batch_size=batch_size,\n",
    "                  epochs=nb_epoch,\n",
    "                  verbose=1, \n",
    "                  validation_split=split, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "        save_model(model, num_fold, modelStr)\n",
    "\n",
    "    print('Start testing............')\n",
    "    test_data, test_id = read_and_normalize_test_data(img_rows, img_cols,\n",
    "                                                      color_type_global)\n",
    "    yfull_test = []\n",
    "\n",
    "    for index in range(1, num_fold + 1):\n",
    "        # 1,2,3,4,5\n",
    "        # Store test predictions\n",
    "        model = read_model(index, modelStr)\n",
    "        test_prediction = model.predict(test_data, batch_size=128, verbose=1)\n",
    "        yfull_test.append(test_prediction)\n",
    "\n",
    "    info_string = 'loss_' + modelStr \\\n",
    "                  + '_r_' + str(img_rows) \\\n",
    "                  + '_c_' + str(img_cols) \\\n",
    "                  + '_folds_' + str(nfolds) \\\n",
    "                  + '_ep_' + str(nb_epoch)\n",
    "\n",
    "    test_res = merge_several_folds_mean(yfull_test, nfolds)\n",
    "    create_submission(test_res, test_id, info_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore train from cache!\n",
      "Restore data from pickle........\n",
      "Finished loading Pickle data\n",
      "Train shape: (67272, 224, 224, 3)\n",
      "67272 train samples\n",
      "Start KFold number 1 of 2\n",
      "Train on 57181 samples, validate on 10091 samples\n",
      "Epoch 1/10\n",
      "57181/57181 [==============================] - 2987s 52ms/step - loss: 2.2572 - acc: 0.2951 - val_loss: 2.1718 - val_acc: 0.4918\n",
      "Epoch 2/10\n",
      "57181/57181 [==============================] - 3049s 53ms/step - loss: 2.0399 - acc: 0.5590 - val_loss: 1.9066 - val_acc: 0.5921\n",
      "Epoch 3/10\n",
      "57181/57181 [==============================] - 2989s 52ms/step - loss: 1.7834 - acc: 0.5979 - val_loss: 1.6657 - val_acc: 0.6026\n",
      "Epoch 4/10\n",
      "57181/57181 [==============================] - 2955s 52ms/step - loss: 1.5655 - acc: 0.6041 - val_loss: 1.4716 - val_acc: 0.6054\n",
      "Epoch 5/10\n",
      "57181/57181 [==============================] - 2979s 52ms/step - loss: 1.3899 - acc: 0.6067 - val_loss: 1.3242 - val_acc: 0.6030\n",
      "Epoch 6/10\n",
      "57181/57181 [==============================] - 2985s 52ms/step - loss: 1.2534 - acc: 0.6082 - val_loss: 1.2002 - val_acc: 0.6083\n",
      "Epoch 7/10\n",
      "57181/57181 [==============================] - 2965s 52ms/step - loss: 1.1479 - acc: 0.6085 - val_loss: 1.1047 - val_acc: 0.6078\n",
      "Epoch 8/10\n",
      "57181/57181 [==============================] - 2953s 52ms/step - loss: 1.0646 - acc: 0.6090 - val_loss: 1.0334 - val_acc: 0.6102\n",
      "Epoch 9/10\n",
      "57181/57181 [==============================] - 3032s 53ms/step - loss: 1.0003 - acc: 0.6092 - val_loss: 0.9748 - val_acc: 0.6105\n",
      "Epoch 10/10\n",
      "57181/57181 [==============================] - 2961s 52ms/step - loss: 0.9491 - acc: 0.6097 - val_loss: 0.9306 - val_acc: 0.6103\n",
      "Model weights saved\n",
      "Start KFold number 2 of 2\n",
      "Train on 57181 samples, validate on 10091 samples\n",
      "Epoch 1/10\n",
      "57181/57181 [==============================] - 2924s 51ms/step - loss: 2.2452 - acc: 0.3474 - val_loss: 2.1351 - val_acc: 0.6432\n",
      "Epoch 2/10\n",
      "57181/57181 [==============================] - 2904s 51ms/step - loss: 1.9783 - acc: 0.6920 - val_loss: 1.8270 - val_acc: 0.7105\n",
      "Epoch 3/10\n",
      "57181/57181 [==============================] - 3072s 54ms/step - loss: 1.6919 - acc: 0.7086 - val_loss: 1.5596 - val_acc: 0.7171\n",
      "Epoch 4/10\n",
      "57181/57181 [==============================] - 2888s 51ms/step - loss: 1.4544 - acc: 0.7113 - val_loss: 1.3495 - val_acc: 0.7180\n",
      "Epoch 5/10\n",
      "57181/57181 [==============================] - 3001s 52ms/step - loss: 1.2658 - acc: 0.7125 - val_loss: 1.1844 - val_acc: 0.7179\n",
      "Epoch 6/10\n",
      "57181/57181 [==============================] - 2951s 52ms/step - loss: 1.1173 - acc: 0.7132 - val_loss: 1.0506 - val_acc: 0.7197\n",
      "Epoch 7/10\n",
      "57181/57181 [==============================] - 2956s 52ms/step - loss: 1.0031 - acc: 0.7133 - val_loss: 0.9507 - val_acc: 0.7194\n",
      "Epoch 8/10\n",
      "57181/57181 [==============================] - 2943s 51ms/step - loss: 0.9147 - acc: 0.7132 - val_loss: 0.8721 - val_acc: 0.7196\n",
      "Epoch 9/10\n",
      "57181/57181 [==============================] - 2984s 52ms/step - loss: 0.8442 - acc: 0.7135 - val_loss: 0.8097 - val_acc: 0.7201\n",
      "Epoch 10/10\n",
      "57181/57181 [==============================] - 3014s 53ms/step - loss: 0.7889 - acc: 0.7138 - val_loss: 0.7639 - val_acc: 0.7188\n",
      "Model weights saved\n",
      "Start testing............\n",
      "Restore test from cache!\n",
      "Restore data from pickle........\n",
      "Finished loading Pickle data\n",
      "Test shape: (79726, 224, 224, 3)\n",
      "79726 test samples\n",
      "Reading model weights\n",
      "79726/79726 [==============================] - 1694s 21ms/step\n",
      "Reading model weights\n",
      "79726/79726 [==============================] - 1844s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "# pass in parameters for nfolds, epochs, split, model string\n",
    "\n",
    "run_cross_validation(2, 10, 0.15, '_vgg_16_2x10')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
